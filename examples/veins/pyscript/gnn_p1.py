# -*- coding: utf-8 -*-
"""GNN-P1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yFpaA4EuoKgg6Z5p4Aj4ygmcZj6fY4T7

import libs
"""

import os
import os.path as osp
import shutil
import pandas as pd
import random
import datetime

# libraries for the files in google drive
# from pydrive.auth import GoogleAuth
# from google.colab import drive
# from pydrive.drive import GoogleDrive
# from google.colab import auth
# from oauth2client.client import GoogleCredentials


import torch
os.environ['TORCH'] = torch.__version__
print(torch.__version__)

# # GPU Usage Guide - https://medium.com/@natsunoyuki/speeding-up-model-training-with-google-colab-b1ad7c48573e
# if torch.cuda.is_available():
#     device_name = torch.device("cuda")
# else:
#     device_name = torch.device('cpu')
# print("Using {}.".format(device_name))
#
# !pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html
# !pip install --verbose torch-scatter
# !pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html
# !pip install -q git+https://github.com/pyg-team/pytorch_geometric.git
# !pip install torch_geometric pandas
# !pip install scikit-learn

from collections import Counter
import matplotlib.pyplot as plt
import math
import networkx as nx
import numpy as np
from scipy.spatial.distance import cdist, squareform
from scipy import stats
from sklearn.cluster import KMeans, MeanShift, AffinityPropagation, FeatureAgglomeration, SpectralClustering, MiniBatchKMeans, Birch, DBSCAN, OPTICS, AgglomerativeClustering
from sklearn.impute import SimpleImputer
from sklearn.mixture import GaussianMixture
from sklearn.metrics import confusion_matrix, pairwise_distances, davies_bouldin_score, silhouette_score, calinski_harabasz_score, adjusted_rand_score, normalized_mutual_info_score
from torch_geometric.data import Data, InMemoryDataset, download_url
from torch_geometric.loader import DataLoader
from torch_geometric.datasets import Planetoid, TUDataset
import torch_geometric.transforms as T
from torch_geometric.nn import GCNConv, SAGEConv, GAE, GINConv, GATConv
from torch_geometric.utils import train_test_split_edges, to_networkx, from_networkx, to_dense_adj
from torch_geometric.transforms import NormalizeFeatures, ToDevice, RandomLinkSplit, RemoveDuplicatedEdges
import torch.nn.functional as F

# !pip install torch-geometric
import torch

from torch_geometric.data import InMemoryDataset, download_url

import torch_geometric.transforms as T

import os
import torch
import pandas as pd
import numpy as np
from torch_geometric.data import InMemoryDataset
from torch_geometric.data import Data
import torch_geometric.transforms as T
import os.path as osp

# """check gpu state"""
#
# gpu_info = !nvidia-smi
# gpu_info = '\n'.join(gpu_info)
# if gpu_info.find('failed') >= 0:
#   print('Not connected to a GPU')
# else:
#   print(gpu_info)
#
# """check memory"""
#
# from psutil import virtual_memory
# ram_gb = virtual_memory().total / 1e9
# print('Your runtime has {:.1f} gigabytes of available RAM\n'.format(ram_gb))
#
# if ram_gb < 20:
#   print('Not using a high-RAM runtime')
# else:
#   print('You are using a high-RAM runtime!')

# Define the root directory where the dataset will be stored
root = './'
version = 'v1'
run_id = 'test'

# Define the Number of Clusters
num_clusters = 6
n = 10 # epoch and n_init refers to the number of times the clustering algorithm will run different initializations
clusters = []

max_dist = 500

# Channel Parameters & GAE MODEL
# in_channels = data.num_features
# out_channels = data.size(0)
# print('features', data.num_features, 'edges', out_channels)

# Transform Parameters
transform_set = True
value_num = 0.1
test_num = 0.2

# Optimizer Parameters (learning rate)
learn_rate = 0.01

# Epochs or the number of generation/iterations of the training dataset
epochs = 500

# Setting up Colours for the test visualizations
colours_simp = ['b', 'g', 'r', 'c', 'm', 'y', 'k']
colors_full = ["black" if c <= -1 else "b" if c == 0 else "g" if c == 1 else "r" if c == 2 else "c" if c == 3 else "m" if c == 4 else "y" if c == 5 else "k" for c in clusters]

# Start Session Time
start_time = datetime.datetime.now()

"""import the data"""

# Import the Data
from google.colab import files
uploaded = files.upload()
# uploaded = https://drive.google.com/file/d/1Ej2lijwlKM7pri5ebQ5hVl-Z5OtvWNbz/view?usp=share_link

file_name = next(iter(uploaded))
print(file_name)
df = pd.read_csv(file_name)
df = df.reset_index(drop=True)

uploaded = files.upload()
# uploaded = https://drive.google.com/file/d/1Ej2lijwlKM7pri5ebQ5hVl-Z5OtvWNbz/view?usp=share_link

file_name = next(iter(uploaded))
print(file_name)
eg = pd.read_csv(file_name)
eg = eg.reset_index(drop=True)

"""set up the data"""

# Create an empty graph and lists
G = nx.Graph()
edge_index = []

#create the graph
for i in range(len(df)):
    G.add_node(i, ID=df.loc[i, 'ID'], X=df.loc[i, 'X'], Y=df.loc[i, 'Y'], RSU=df.loc[i, 'RSU'])

for i in range(len(eg)):
    G.add_edge(eg.loc[i, 'node1'], eg.loc[i, 'node2'])

# print the graph
print(G.edges)
print(G)



# !pip install torch-geometric
import torch

from torch_geometric.data import InMemoryDataset, download_url

import torch_geometric.transforms as T

import os
import torch
import pandas as pd
import numpy as np
from torch_geometric.data import InMemoryDataset
from torch_geometric.data import Data
import torch_geometric.transforms as T
import os.path as osp

"""In memory dataset"""

#InMemoryDataset
class MyDataset(InMemoryDataset):
    def __init__(self, root, file_name, transform=None, pre_transform=None):
        self.filename = file_name
        super().__init__(root, transform, pre_transform)
        ## if the data exists in the processed dir, then it loads directly from there, if not, it will run process
        self.data = torch.load(self.processed_paths[0])

    @property
    def raw_file_names(self):
        return self.filename

    @property
    def processed_file_names(self):
       return ['data.pt']

    def download(self):
        #get the data from the original folder and save it to the raw folder(node data)
        from_path0 = os.path.join(self.root, str(self.filename[0]))
        to_path0 = dest = os.path.join(self.raw_dir, str(self.filename[0]))
        df = pd.read_csv(from_path0)
        df.reset_index(drop=True)
        df.to_csv(to_path0,index=False)
        #get edge data
        from_path1 = os.path.join(self.root, str(self.filename[1]))
        to_path1 = dest = os.path.join(self.raw_dir, str(self.filename[1]))
        eg = pd.read_csv(from_path1)
        eg.reset_index(drop=True)
        eg.to_csv(to_path1,index=False)

    def read_file(self):
        #read file from raw folder, and clean it
        path0 = osp.join(self.raw_dir, str(self.filename[0]))
        path1 = osp.join(self.raw_dir, str(self.filename[1]))
        df = pd.read_csv(path0)
        df = df.reset_index(drop=True)
        eg = pd.read_csv(path1)
        eg = eg.reset_index(drop=True)

        # Set the RSU and Tower to represent Infrastructure = 1, roads = 0
        # df['Type'] = np.where((df['ID'].str.contains('Tower')) | (df['ID'].str.contains('RSU')), 1, 0)
        df = df.drop(columns=['ID'])

        data = [df,eg]
        return data

    def process(self):
        #read the file and transform it into a pytorch geometric Data
        data = self.read_file()
        df = data[0]
        eg = data[1]
        print(df.head())
        x = torch.from_numpy(df.values)

        edge_source = []
        edge_target = []

        # Loop row by row to add the edges to the graph
        for i in range(len(eg)):
          edge_source.append(eg.iloc[i]['node1'])
          edge_target.append(eg.iloc[i]['node2'])

        # Create PyTorch Geometric Data object
        data = Data(
            x=x,
            edge_index=torch.tensor([edge_source, edge_target])
            )
        torch.save(data, self.processed_paths[0])

# My Dataset code
# Define the root directory where the dataset will be stored
root = './'
file_name = ['node_data+rsu.csv', 'edge_data.csv']

dataset = MyDataset(root,file_name,transform=T.NormalizeFeatures())
print(dataset[0])
print('Done')

data_ = dataset[0]
print(data_)

G = to_networkx(data_)
G = G.to_undirected()

X = data_.x[:,[0,1]].cpu().detach().numpy()
pos = dict(zip(range(X[:, 0].size), X))


# Draw the Graph
fig, ax = plt.subplots(figsize=(10, 10))
ax.scatter(X[:,0], X[:,1], s=20, color='grey')
nx.draw_networkx_nodes(G, pos, node_color='black', node_size=20, ax=ax)
nx.draw_networkx_edges(G, pos, edge_color='grey', ax=ax)
ax.set_xlabel('X')
ax.set_ylabel('Y')
plt.show()

"""LOAD DATA"""

# My Dataset code
# Define the root directory where the dataset will be stored
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

transform = RandomLinkSplit(
    num_val=0.1, num_test=0.2,
    is_undirected=True,
    split_labels=True
    )

#dataset = Planetoid(root='data/Planetoid', name='Citeseer', transform=NormalizeFeatures())
data = dataset[0]
train_data, val_data, test_data = transform(data)

# Create adjacency matrix
A = {}
edge_index = train_data.edge_index
for i in range(edge_index.shape[1]):
    src = edge_index[0, i].item()
    tgt = edge_index[1, i].item()
    if src not in A:
        A[src] = []
    A[src].append(tgt)

print('adjacency matrix:', A)

# Print information
print('dataset',dataset)
print('data', data)
print('train data',train_data)
print('test data', test_data)
print('------------')
train_pos_edge_index = train_data.pos_edge_label_index
print(train_pos_edge_index.dtype)
print(test_data.neg_edge_label_index.dtype)
print('training positive edges:', train_pos_edge_index)
print('------------')
print(f'Number of graphs: {len(dataset)}')
print(f'Number of features: {dataset.num_features}')
print('------------')

device = 'cuda' if torch.cuda.is_available() else 'cpu'

# If you use GPU, the device should be cuda
print('Device: {}'.format(device))

"""define encoder"""

class GCNEncoder(torch.nn.Module):
    def __init__(self, in_channels, out_channels):
        super(GCNEncoder, self).__init__()
        #Defines the GNN Layers
        print('in channels', in_channels)
        print('out channels', out_channels)
        self.conv1 = SAGEConv(in_channels, out_channels, cached=True) # cached only for transductive learning
        self.conv2 = SAGEConv(out_channels, in_channels, cached=True) # cached only for transductive learning

    def forward(self, x, edge_index):
        x = x.float()
        x = self.conv1(x, edge_index).relu()
        return self.conv2(x, edge_index)

"""define autoencoder"""

# Channel Parameters & GAE MODEL
in_channels = data.num_features
out_channels = 50
print('features', data.num_features, 'edges', out_channels)


# Initialize the model
model = GAE(GCNEncoder(in_channels, out_channels))

# move to GPU (if available)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('device', device)

model = model.to(device).float()
data = dataset[0].to(device)
x = train_data.x.to(device).float()
# train_pos_edge_index = train_data.pos_edge_label_index.to(device)
train_pos_edge_index = train_pos_edge_index.to(device)

# Define the loss function
criterion = torch.nn.CrossEntropyLoss()

# inizialize the optimizer
optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)

def train():
    model.train()
    optimizer.zero_grad()
    z = model.encode(x, train_pos_edge_index)
    loss = model.recon_loss(z, train_pos_edge_index)
    loss.backward()
    optimizer.step()
    return float(loss), z


def test(pos_edge_index, neg_edge_index):
    model.eval()
    with torch.no_grad():
      x = data.x.to(device)
      pos_edge_index = pos_edge_index.to(device)
      z = model.encode(x, pos_edge_index)
    return model.test(z, pos_edge_index, neg_edge_index)

print(model)

for epoch in range(1, 500):
    loss, z = train()
    auc, ap = test(test_data.pos_edge_label_index, test_data.neg_edge_label_index)
    print('Epoch: {:03d}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, auc, ap))

#perform DBSCAN
d = DBSCAN(eps=0.15).fit(z.cpu().detach().numpy())
print(d.labels_)
colors = colours_simp
for i in range(num_clusters):
    print(z[0].cpu().detach().numpy())
    plt.scatter(z[d.labels_ == i, 0].cpu().detach().numpy(),
                z[d.labels_ == i, 1].cpu().detach().numpy(),
                c=colors[i % len(colours_simp)], alpha=0.5)

plt.show()

# draw the graph
fig, ax = plt.subplots(figsize=(8, 8))
colors = colours_simp
# Color nodes based on the cluster they belong to
node_colors = [colors[i % len(colors)] for i in d.labels_]
nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=20, ax=ax)


# Draw the remaining elements
ax.scatter(df['X'], df['Y'], s=20, color='black')
ax.scatter(900, 900, s=100, color="black")
ax.scatter(3300, 900, s=100, color="black")
ax.scatter(900, 2500, s=100, color="black")
ax.scatter(3300, 2500, s=100, color="black")
ax.scatter(1600, 600, s=100, color="black")
ax.scatter(900, 1600, s=100, color="black")
ax.set_xlabel('X')
ax.set_ylabel('Y')

# Show the plot
plt.show()

# Perform KMeans clustering on the embeddings
kmeans = KMeans(n_clusters=6, n_init=n).fit(z.cpu().detach().numpy())

# End Time
end_time = datetime.datetime.now()

# Plot the clusters
colors = colours_simp
for i in range(num_clusters):
    plt.scatter(z[kmeans.labels_ == i, 0].cpu().detach().numpy(),
                z[kmeans.labels_ == i, 1].cpu().detach().numpy(),
                c=colors[i % len(colours_simp)], alpha=0.5)
# c=colors[i % len(colours_simp)]
plt.scatter(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], s=100, color="red")
plt.show()

# draw the graph
fig, ax = plt.subplots(figsize=(8, 8))
print(kmeans.labels_)
# Color nodes based on the cluster they belong to
node_colors = [colors[i % len(colors)] for i in kmeans.labels_]
nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=20, ax=ax)


# Draw the remaining elements
ax.scatter(df['X'], df['Y'], s=20, color='black')
ax.scatter(900, 900, s=200, color="black")
ax.scatter(3300, 900, s=200, color="black")
ax.scatter(900, 2500, s=200, color="black")
ax.scatter(3300, 2500, s=200, color="black")
ax.scatter(1600, 600, s=200, color="black")
ax.scatter(900, 1600, s=200, color="black")
ax.set_xlabel('X')
ax.set_ylabel('Y')

# Show the plot
plt.show()



# # ---------------------------------------------Anthony
# x_y_array = []

# for i in range(G.number_of_nodes()):
#   x_y_array.append([G.nodes.data('x')[i], G.nodes.data('y')[i]])


# # Run K-means clustering on the x-y matrix
# kmeans = KMeans(n_clusters=num_clusters, n_init=n).fit(x_y_array)

# # Assign the nodes to the corresponding clusters
# clusters = kmeans.labels_

# # Plot Graph with Clusters
# plt.figure(figsize=(8, 8))
# # pos = nx.spring_layout(G)

# pos = {}
# for i in range(G.number_of_nodes()):
#   pos[i] = (x_y_array[i][0], x_y_array[i][1])

# for i, cluster in enumerate(clusters):
#     color = 'black' if i in range(6) else 'grey' if i in range(6, 10) else plt.cm.Set1(cluster) # RSU's black, Towers grey
#     nx.draw_networkx_nodes(G, pos, [i], node_size=60, node_color=color)
# nx.draw_networkx_edges(G, pos, alpha=0.1)
# plt.show()